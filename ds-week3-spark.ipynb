{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "## Part 1\n",
    "Now let's calculate covariance and correlation by ourselves using ApacheSpark\n",
    "\n",
    "1st we crate two random RDDâ€™s, which shouldn't correlate at all.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 61, 39, 62, 79, 45, 1, 88, 4, 23]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "rddX = sc.parallelize(random.sample(range(100),100))\n",
    "print rddX.take(10)\n",
    "rddY = sc.parallelize(random.sample(range(100),100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we calculate the mean, note that we explicitly cast the denominator to float in order to obtain a float instead of int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.5\n",
      "49.5\n"
     ]
    }
   ],
   "source": [
    "meanX = rddX.sum()/float(rddX.count())\n",
    "meanY = rddY.sum()/float(rddY.count())\n",
    "print meanX\n",
    "print meanY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we calculate the covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-89.61"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddXY = rddX.zip(rddY)\n",
    "covXY = rddXY.map(lambda (x,y): (x-meanX)*(y-meanY)).sum()/float(rddXY.count())\n",
    "covXY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covariance is not a normalized measure. Therefore we use it to calculate correlation. But before that we need to calculate the indivicual standard deviations first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.8660700477\n",
      "28.8660700477\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "n = rddXY.count()\n",
    "sdX = sqrt(rddX.map(lambda x : pow(x-meanX,2)).sum()/n)\n",
    "sdY = sqrt(rddY.map(lambda x : pow(x-meanY,2)).sum()/n)\n",
    "print(sdX)\n",
    "print(sdY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we calculate the correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.10754275427542755"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrXY = covXY / (sdX * sdY)\n",
    "corrXY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-89.61\n"
     ]
    }
   ],
   "source": [
    "def covariance(rdd1, rdd2):\n",
    "    mean1 = rdd1.sum()/float(rdd1.count())\n",
    "    mean2 = rdd2.sum()/float(rdd2.count())\n",
    "    rdd12 = rdd1.zip(rdd2)\n",
    "    return rdd12.map(lambda (x,y): (x-mean1)*(y-mean2)).sum()/float(rdd12.count())\n",
    "\n",
    "print covariance(rddX, rddY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.107542754275\n",
      "(-89.61, -0.10754275427542755)\n"
     ]
    }
   ],
   "source": [
    "def correlation(rdd1, rdd2):\n",
    "    mean1 = rdd1.sum()/float(rdd1.count())\n",
    "    mean2 = rdd2.sum()/float(rdd2.count())\n",
    "    sd1 = sqrt(rdd1.map(lambda x : pow(x-mean1,2)).sum()/rdd1.count())\n",
    "    sd2 = sqrt(rdd2.map(lambda x : pow(x-mean2,2)).sum()/rdd2.count())\n",
    "    return covariance(rdd1, rdd2)/(sd1 * sd2)\n",
    "\n",
    "print correlation(rddX, rddY)\n",
    "\n",
    "def covariance_correlation(rdd1, rdd2):\n",
    "    mean1 = rdd1.sum()/float(rdd1.count())\n",
    "    mean2 = rdd2.sum()/float(rdd2.count())\n",
    "    sd1 = sqrt(rdd1.map(lambda x : pow(x-mean1,2)).sum()/rdd1.count())\n",
    "    sd2 = sqrt(rdd2.map(lambda x : pow(x-mean2,2)).sum()/rdd2.count())\n",
    "    rdd12 = rdd1.zip(rdd2)\n",
    "    cov = rdd12.map(lambda (x,y): (x-mean1)*(y-mean2)).sum()/float(rdd12.count())\n",
    "    return cov, cov/(sd1 * sd2)\n",
    "\n",
    "print covariance_correlation(rddX, rddY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "No we want to create a correlation matrix out of the four RDDs used in the lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          1.         -1.          0.00957696]\n",
      " [ 1.          1.         -1.          0.00957696]\n",
      " [-1.         -1.          1.         -0.00957696]\n",
      " [ 0.00957696  0.00957696 -0.00957696  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.stat import Statistics\n",
    "import random\n",
    "column1 = sc.parallelize(range(100))\n",
    "column2 = sc.parallelize(range(100,200))\n",
    "column3 = sc.parallelize(list(reversed(range(100))))\n",
    "column4 = sc.parallelize(random.sample(range(100),100))\n",
    "data = column1.zip(column2).zip(column3).zip(column4).map(lambda (((a,b),c),d) : (a,b,c,d) ).map(lambda (a,b,c,d) : [a,b,c,d])\n",
    "print Statistics.corr(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "[[ 1.          0.70927291]\n",
      " [ 0.70927291  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "column5 = sc.parallelize([1,2,3,4,5,6,7,8,9,10])\n",
    "print column5.take(10)\n",
    "column6 = sc.parallelize([7,6,5,4,5,6,7,8,9,10])\n",
    "\n",
    "data = column5.zip(column6).map(lambda (x,y):[x,y])\n",
    "print Statistics.corr(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.65\n",
      "0.709272912084\n"
     ]
    }
   ],
   "source": [
    "print covariance(column5, column6)\n",
    "print correlation(column5, column6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0, 0.0)\n"
     ]
    }
   ],
   "source": [
    "print covariance_correlation(sc.parallelize([1,2,3,4,5,6,7]), sc.parallelize([7,6,5,4,5,6,7]))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2 with Spark 2.1",
   "language": "python",
   "name": "python2-spark21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
